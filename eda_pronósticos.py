# -*- coding: utf-8 -*-
"""EDA Pronósticos.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1DjnBPTgApK-TxfzB9ik11pFrHYVEhC4r

[AcPcsMl].[Ml].[Vta_Corregida_Colombia]

select * from [AcpcsMl].[Ml].[Vta_Corregida_Colombia_v]
"""

pip install pyod

# import pyodbc #para conexion a sql
import pandas as pd
import numpy as np
import seaborn as sns #graficar puntos faltantes
import matplotlib.pyplot as plt #gaficar boxplot
import warnings
warnings.filterwarnings('ignore')
from pyod.models.knn import KNN

import pandas as pd

fecha_str = "01.2022"
fecha_datetime = pd.to_datetime(fecha_str + '-1', format='%W.%Y-%w')

fecha_datetime

"""# Prueba EDA"""

fecha_datetime

df=pd.read_excel(r"/content/Venta Corregida OK.xlsx",sheet_name="10000")

df['Fecha Corregida']=df['Fecha Corregida'].astype('str')

df.info()

df['Fecha Corregida'].unique()

# Convertir la columna de cadenas de texto a objetos DateTime
df['fecha_datetime'] = pd.to_datetime(df['Fecha Corregida'] + '-1', format='%Y-%W-%w')

# Ordenar el DataFrame por fecha
df = df.sort_values('fecha_datetime')
df

df_nn13=df[df['Centro Distribución']=="NN13"]

df_nn13

import plotly.express as px

fig=px.line(df_nn13, x='fecha_datetime',y='Cantidad', title = 'Historico mat 1000000')

fig.update_xaxes(rangeslider_visible=True)
fig.show()

fig=px.line(df_nn13, x='fecha_datetime',y='Cantidad', title = 'Historico mat 1000000')

fig.update_xaxes(
    rangeslider_visible=True,
    rangeselector=dict(
        buttons=list([
                      dict(count=1, label='1y',step="year", stepmode="backward"),
                      dict(count=1, label='1M',step='month', stepmode="backward"),
                      dict(count=1, label='all',step='all', stepmode="backward")
                     ])
    ))
fig.show()

import plotly.graph_objs as go
from plotly.subplots import make_subplots

df_nn132020=df_nn13[df_nn13['Año']==2020]
df_nn132021=df_nn13[df_nn13['Año']==2021]
df_nn132022=df_nn13[df_nn13['Año']==2022]
df_nn132023=df_nn13[df_nn13['Año']==2023]

fig = make_subplots(specs=[[{"secondary_y": True}]])

fig.add_trace(go.Scatter(x=df_nn132020['Semana'],y=df_nn132020['Cantidad'],
                         mode='lines', line=dict(color='red'), legendrank=True, name="2020"))
fig.add_trace(go.Scatter(x=df_nn132021['Semana'],y=df_nn132021['Cantidad'], 
                         mode='lines', line=dict(color='blue'), legendrank=True, name="2021"))
fig.add_trace(go.Scatter(x=df_nn132022['Semana'],y=df_nn132022['Cantidad'], 
                         mode='lines', line=dict(color='black'), legendrank=True, name="2022"))
fig.add_trace(go.Scatter(x=df_nn132023['Semana'],y=df_nn132023['Cantidad'], 
                         mode='lines', line=dict(color='orange'), legendrank=True, name="2023"))

# fig.update_xaxes(
#     rangeslider_visible=True,
#     rangeselector=dict(
#         buttons=list([
#                       dict(count=1, label='1y',step="year", stepmode="backward"),
#                       dict(count=1, label='1M',step='month', stepmode="backward"),
#                       dict(count=1, label='all',step='all', stepmode="backward")
#                      ])
#     ))
fig.show()

df_nn132021_2=df_nn13[(df_nn13['Año']==2021) | (df_nn13['Año']==2022)]

df_nn132021_2.head()

df_nn132021_2[['Semana','Cantidad']].groupby('Semana').describe()

df_nn132021_2[['Semana','Cantidad']].groupby('Semana').agg({'Cantidad':['max','min','std']})

df_nn97=df[df['Centro Distribución']=="NN97"]

df_nn14=df[df['Centro Distribución']=="NN14"]

df_nn14.Cantidad.hist()

df_nn97['Cantidad'].hist()

df_nn13['Cantidad'].hist()

df_nn13['Cantidad'].plot(kind='density')

df_nn13_dt=df_nn13.set_index('fecha_datetime')

df_nn13_dt.index

pd.plotting.lag_plot(df_nn13_dt['Cantidad'],lag=51)

df_semanas=df_nn132021[['Semana','Cantidad']].merge(df_nn132022[['Semana','Cantidad']],on='Semana',how='left').merge(df_nn132020[['Semana','Cantidad']],on='Semana',how='left')

df_semanas.iloc[9:]

import seaborn as sns

sns.pairplot(df_semanas.iloc[9:][['Cantidad_x','Cantidad_y','Cantidad']])

correlation=df_semanas.iloc[9:][['Cantidad_x','Cantidad_y','Cantidad']].corr(method='pearson')

g=sns.heatmap(correlation,square=True,cbar_kws={"shrink": .5},annot=True,fmt='.2f',cmap="coolwarm")
g.figure.set_size_inches(10,10)

plt.show()

pd.plotting.autocorrelation_plot(df_nn13_dt[(df_nn13_dt['Año']==2021) | (df_nn13_dt['Año']==2022)]['Cantidad'])

df_nn13_dt['Cantidad'].resample("1m").mean()

pd.plotting.autocorrelation_plot(df_nn13_dt[(df_nn13_dt['Año']==2021) | (df_nn13_dt['Año']==2022)]['Cantidad'].resample("1m").mean())

"""# EDA Con los Datos Reales"""

df=pd.read_excel(r"/content/Pronosticos_.xlsx",sheet_name="Hoja1")

df.info()

df.head()

#Restructura la tabla 
dfs=df.pivot(index='semana',columns="Codmat",values='cant')
dfs

#identificar faltaltes en las series (azul oscuro zonas con datos faltantes)
sns.heatmap(dfs.isnull(), yticklabels=False, cbar=False, cmap = "Blues")

"""### Restricción últimas 4 semanas
* Existen materiales que no tienen información de venta de las ultimas 4 semanas. Se decide seleccionar estos materiales y socializarlos con el área de negocio (regla 1).
"""

#Creterio: Excluir series que le falten las últimas 4 semanas y revisar en detalle
w4=dfs[-4:].isnull().sum()
sem4=w4[w4==4]
fw4=pd.DataFrame(sem4.index)

#materiales que cumplen
fw=w4[w4<4]
fw=pd.DataFrame(fw.index)
#fw

#seleccionamos del dataframe orignal los que sirven
dfp_2=pd.merge(df,fw,how='inner',left_on='Codmat',right_on='Codmat')
dfp_2

#actualizar series de tiempo con información reciente
dfs_2=dfs.drop(sem4.index,axis=1)
dfs_2

"""## Completitud de la serie

* Existen materiales que presentan un porcentaje de valores perdidos mayor al 20% se recomienda revisar estas series (regla 2).
"""

#dfp_2.groupby(['codmat']).semana.agg({'Minimun_Date': "min",  'Maximun_Date': "max",'Cant_sem':np.count_nonzero})
dfcomp=dfp_2.groupby('Codmat')['semana'].agg(Minimun_Date='min',Maximun_Date='max',Cant_sem=np.count_nonzero)
#convertir columnas a fecha
dfcomp['Maximun_Date'] = pd.to_datetime(dfcomp['Maximun_Date'])
dfcomp['Minimun_Date']=pd.to_datetime(dfcomp['Minimun_Date'])

#pasara a semanas de direrencia
dfcomp['sem_rang']=np.floor((dfcomp['Maximun_Date']-dfcomp['Minimun_Date']).dt.days)/7

#procentajes de completitud
dfcomp['porcomp']=dfcomp['Cant_sem']/dfcomp['sem_rang']
#mostrar data frame
dfcomp

#Filtro series >= 80%
m1=dfcomp['porcomp']>=0.7
m2=dfcomp['porcomp']<0.7

dfs_comp=dfcomp[m1].index
dfs_comp

#Actualizar series con completidud mayor o igual al 80%
dfs_3=dfs_2[dfs_comp]
dfs_3

fw2=pd.DataFrame(dfs_3.columns)
dfp_3=pd.merge(dfp_2,fw2,how='inner',left_on='Codmat',right_on='Codmat')
dfp_3

"""## Secuencia de NA en la serie

* Una vez se depuran aquellos materiales con una completitud menor al 80%, se encontró series con valores perdidos en secuencia, los cual dificultad el proceso de imputación de valores perdidos. Se recomienda revisar estas series. (Regla3)
"""

#Encontrar secuencias de NA en la serie 

#listas para guardar los resultados
racha=[]
racha_count=[]
nulos=[]
l=range(0,dfs_3.shape[1])
#Bluce para recorrer la cantidad de series
for i in l:
  #extracción de la serie
  serie=pd.DataFrame(dfs_3.iloc[:,i]).astype(str)
  cod=serie.columns[0]
 
  rf=dfcomp[dfcomp.index==cod]
  m3=serie.index>=min(rf['Minimun_Date'].astype(str))
  serie=serie.iloc[m3]

  #Conteo de racha
  serie['racha']=serie.iloc[:,0].groupby((serie.iloc[:,0]!=serie.iloc[:,0].shift()).cumsum()).cumcount()+1
  secue_na=serie[serie.iloc[:,0]=='nan']['racha'].max()
    
  #Guardar los resultados
  racha.append(cod)
  nulos.append((serie.iloc[:,0]=='nan').sum())
  racha_count.append(secue_na)

secuencia=pd.DataFrame(nulos,racha)
secuencia['Racha_Count']= racha_count
secuencia.columns=['Nulos','Racha_Nulos']
secuencia=secuencia.fillna(0)
secuencia

#unir todo en un solo dataframe
dfcomp_2=pd.merge(dfcomp,secuencia,how='inner',left_index=True,right_index=True)
dfcomp_2

#extraer rachas mayores a 3 
dfs_4=  dfs_3[dfcomp_2[dfcomp_2['Racha_Nulos']<=3].index]
dfs_iqr=dfs_3[dfcomp_2[dfcomp_2['Racha_Nulos']<=3].index]
#dfs_1sd=dfs_3[dfcomp_2[dfcomp_2['Racha_Nulos']<=3].index]
#dfs_2sd=dfs_3[dfcomp_2[dfcomp_2['Racha_Nulos']<=3].index]
#dfs_3sd=dfs_3[dfcomp_2[dfcomp_2['Racha_Nulos']<=3].index]

#covertir indice en datetime
dfs_4.index = pd.to_datetime(dfs_4.index)
dfs_iqr.index=pd.to_datetime(dfs_iqr.index)
dfs_4.head(5)

"""#Tratamiento de datos atípicos"""

#Funciones para calcular las series con tratamiento de datos atípicos 
# Metodo del rango intercuartilico Q3+1.5(Q3-Q1) y Q1-1.5(Q3-Q1)
# Metos de la media + 1(68.3%),2(95.4%) y 3(99.7) desviaciones estandar 

def iqr(x):
  #atípicos por IQR
    #print(type(x))
    #x=pd.DataFrame(x)
    Q1=x.quantile(q=0.25)
    Q3=x.quantile(q=0.75)
    IQR=Q3-Q1
    LS=Q3+1.5*IQR
    LI=Q1-1.5*IQR
    m4=x>LS
    m5=x<LI
    #print(x.dtype  )
    #asigna limte superior a los que se salieron del rango
    for material in list(x.columns.values):
        x.loc[m4[material],material]=LS[material]
        
    for material in list(x.columns.values):
        x.loc[m5[material],material]=LI[material]   
        
    return x

#resultados de las series 
#dfs_iqr=dfs_iqr.astype(float)
#dfs_iqr.iloc[:, 0:2].apply(iqr,axis=0)
dfs_iqr.transform(iqr,axis=0)
dfs_iqr=dfs_iqr.rename(columns=lambda x: str(x) + '_IQR')

#dfs_1sd.iloc[:, 0:1].apply(sd1,axis=0)
#dfs_1sd=dfs_1sd.rename(columns=lambda x: str(x) + '_1sd')

#dfs_2sd.apply(sd2,axis=0)
#dfs_2sd=dfs_2sd.rename(columns=lambda x: str(x) + '_2sd')

#dfs_3sd.apply(sd3,axis=0)
#dfs_3sd=dfs_3sd.rename(columns=lambda x: str(x) + '_3sd')

dfs_iqr.index.max()

import plotly.graph_objects as go
def Tendencia (X, Y, TituloGraf, LeyendaX, LeyendaY, ): 

    fig = go.Figure(layout=go.Layout(xaxis={'type': 'category','title':LeyendaX} ,
                                 title = TituloGraf,
                                 yaxis = dict(title = LeyendaY)))
    fig.add_trace(go.Scatter(x=X, y=Y,
                    mode='lines+markers', 
                    #name='Ventas',
                    #showlegend  =  True
                       
                       ))


    fig.show()

dfs=pd.merge(dfs_4,dfs_iqr,how='inner', left_index=True, right_index=True)

#fecha minimas y maximas para los 5 escenarios 
dfcomp_f=dfp_2.groupby(['Codmat'])
dfcomp_f=dfcomp_f.agg(Minimun_Date=('semana',np.min),Maximun_Date=('semana',np.max),Cant_sem=('semana',np.count_nonzero))

dfcomp_iqr=dfp_2.groupby(['Codmat'])
dfcomp_iqr=dfcomp_iqr.agg(Minimun_Date=('semana',np.min),Maximun_Date=('semana',np.max),Cant_sem=('semana',np.count_nonzero))
dfcomp_iqr=dfcomp_iqr.rename(index=lambda x: str(x) + '_IQR')

dfcomp_f=dfcomp_f.append(dfcomp_iqr)
#dfcomp_f=dfcomp_f.append(dfcomp_1sd)
#dfcomp_f=dfcomp_f.append(dfcomp_2sd)
#dfcomp_f=dfcomp_f.append(dfcomp_3sd)
dfcomp_f

dfs

"""# EDA"""

fecha_str = "01.2022"
fecha_datetime = pd.to_datetime(fecha_str + '-1', format='%W.%Y-%w')

dfp_3['Semana Corregida']=pd.to_datetime(dfp_3['Semana'] + '-1', format='%Y.%W-%w')

dfs.info()

dfs

sns.pairplot(dfs.iloc[:,1:20])

correlation=dfs.iloc[:,1:20].corr(method='pearson')
g=sns.heatmap(correlation,square=True,cbar_kws={"shrink": .5},annot=True,fmt='.2f',cmap="coolwarm")
g.figure.set_size_inches(20,20)

plt.show()

dfs.index

import plotly.express as px
import plotly.graph_objs as go
from plotly.subplots import make_subplots
#fig=px.line(dfs, x=dfs.index,y='NN13-1012346', title = 'NN13-1012346 VS NN13-1019943')

fig = make_subplots(specs=[[{"secondary_y": True}]])

fig.add_trace(go.Scatter(x=dfs.index,y=dfs['NN13-1012346'],
                         mode='lines', line=dict(color='red'), legendrank=True, name="NN13-1012346"))
fig.add_trace(go.Scatter(x=dfs.index,y=dfs['NN13-1019943'], 
                         mode='lines', line=dict(color='blue'), legendrank=True, name="NN13-1019943"))

#fig.update_xaxes(rangeslider_visible=True)
fig.show()

dfs['NN13-1012346'].hist()

pd.plotting.lag_plot(dfs['NN13-1012346'],lag=50)

dfs['NN13-1019943'].hist()

pd.plotting.autocorrelation_plot(dfs['NN13-1012346'])

pd.plotting.autocorrelation_plot(dfs['NN13-1012346'].resample("50w").mean())

dfs

dfs_2=dfs.reset_index()
dfs_2

codmats=dfs_2.columns.values[1:]
codmats

dfs_2['semana']

dfs_2.columns.values[1:].tolist()

dfs_long = pd.melt(dfs_2, id_vars=dfs_2.columns.values[:1], value_vars=dfs_2.columns.values[1:])

dfs_long

df_semanaAgrupado =df.groupby(['semana','semanacalendario'])
df_semanaAgrupado

dfs_long_w=dfs_long.merge(df,on='semana',how='left')

dfs_long_w